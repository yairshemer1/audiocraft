{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from audiocraft.modules.conv import *\n",
    "from audiocraft.modules.lstm import *\n",
    "from audiocraft.modules.seanet import SEANetResnetBlock2d, SEANetResnetBlock\n",
    "import typing as tp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Printer(nn.Module):\n",
    "\n",
    "    def __init__(self, cls):\n",
    "        super().__init__()\n",
    "        self.cls = str(cls)\n",
    "        self.id = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"{self.cls}: {x.shape}\")\n",
    "        x = self.id(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SEANetEncoder2d(nn.Module):\n",
    "    \"\"\"SEANet encoder.\n",
    "\n",
    "    Args:\n",
    "        channels (int): Audio channels.\n",
    "        dimension (int): Intermediate representation dimension.\n",
    "        n_filters (int): Base width for the model.\n",
    "        n_residual_layers (int): nb of residual layers.\n",
    "        ratios (Sequence[int]): kernel size and stride ratios. The encoder uses downsampling ratios instead of\n",
    "            upsampling ratios, hence it will use the ratios in the reverse order to the ones specified here\n",
    "            that must match the decoder order. We use the decoder order as some models may only employ the decoder.\n",
    "        activation (str): Activation function.\n",
    "        activation_params (dict): Parameters to provide to the activation function.\n",
    "        norm (str): Normalization method.\n",
    "        norm_params (dict): Parameters to provide to the underlying normalization used along with the convolution.\n",
    "        kernel_size (int): Kernel size for the initial convolution.\n",
    "        last_kernel_size (int): Kernel size for the initial convolution.\n",
    "        residual_kernel_size (int): Kernel size for the residual layers.\n",
    "        dilation_base (int): How much to increase the dilation with each layer.\n",
    "        causal (bool): Whether to use fully causal convolution.\n",
    "        pad_mode (str): Padding mode for the convolutions.\n",
    "        true_skip (bool): Whether to use true skip connection or a simple\n",
    "            (streamable) convolution as the skip connection in the residual network blocks.\n",
    "        compress (int): Reduced dimensionality in residual branches (from Demucs v3).\n",
    "        lstm (int): Number of LSTM layers at the end of the encoder.\n",
    "        disable_norm_outer_blocks (int): Number of blocks for which we don't apply norm.\n",
    "            For the encoder, it corresponds to the N first blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 3,\n",
    "                 ratios: tp.List[int] = [8, 5, 4, 2], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},\n",
    "                 norm: str = 'none', norm_params: tp.Dict[str, tp.Any] = {}, kernel_size: int = 7,\n",
    "                 last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False,\n",
    "                 pad_mode: str = 'reflect', true_skip: bool = True, compress: int = 2, lstm: int = 0,\n",
    "                 disable_norm_outer_blocks: int = 0, \n",
    "                 frequency_bins: int = 512, \n",
    "                 num_1x1_convolutions: int = 2,\n",
    "                 temporal_ratios: tp.List[int] = [2]):\n",
    "        super().__init__()\n",
    "        self.num_1x1_convolutions = num_1x1_convolutions\n",
    "        self.temporal_ratios = temporal_ratios\n",
    "        self.channels = channels\n",
    "        self.dimension = dimension\n",
    "        self.n_filters = n_filters\n",
    "        self.frequency_bins = frequency_bins\n",
    "        assert self.frequency_bins == 512, \"model handles 512 frequency channels, change manually and in config file\" \n",
    "        self.ratios = list(reversed(ratios))\n",
    "        del ratios\n",
    "        self.n_residual_layers = n_residual_layers\n",
    "        self.hop_length = np.prod(self.ratios)\n",
    "        self.n_blocks = len(self.ratios) + 2  # first and last conv + residual blocks\n",
    "        self.disable_norm_outer_blocks = disable_norm_outer_blocks\n",
    "        assert self.disable_norm_outer_blocks >= 0 and self.disable_norm_outer_blocks <= self.n_blocks, \\\n",
    "            \"Number of blocks for which to disable norm is invalid.\" \\\n",
    "            \"It should be lower or equal to the actual number of blocks in the network and greater or equal to 0.\"\n",
    "\n",
    "        act = getattr(nn, activation)\n",
    "\n",
    "        chnls = n_filters\n",
    "        freq_bins = frequency_bins\n",
    "\n",
    "        model: tp.List[nn.Module] = [\n",
    "            StreamableConv2d(2, chnls, kernel_size,\n",
    "                             norm='none' if self.disable_norm_outer_blocks >= 1 else norm,\n",
    "                             norm_kwargs=norm_params, causal=causal, pad_mode=pad_mode)\n",
    "        ]\n",
    "        # Take strides accross frequency dimension\n",
    "        for i, ratio in enumerate(self.ratios):\n",
    "            block_norm = 'none' if self.disable_norm_outer_blocks >= i + 2 else norm\n",
    "            # Add residual layers\n",
    "            for j in range(n_residual_layers):\n",
    "                model += [\n",
    "                    SEANetResnetBlock2d(chnls, kernel_sizes=[residual_kernel_size, 1],\n",
    "                                        dilations=[dilation_base ** j, 1],\n",
    "                                        norm=block_norm, norm_params=norm_params,\n",
    "                                        activation=activation, activation_params=activation_params,\n",
    "                                        causal=causal, pad_mode=pad_mode, compress=compress, true_skip=true_skip)]\n",
    "\n",
    "            # Add downsampling layers\n",
    "            model += [\n",
    "                act(**activation_params),\n",
    "                StreamableConv2d(chnls, chnls * 2,\n",
    "                                 kernel_size=ratio * 2, stride=ratio,\n",
    "                                 norm=block_norm, norm_kwargs=norm_params,\n",
    "                                 causal=causal, pad_mode=pad_mode),\n",
    "            ]\n",
    "            chnls *= 2\n",
    "            freq_bins //= 2\n",
    "\n",
    "        # decrease channels using 1x1 convolutions\n",
    "        for _ in range(self.num_1x1_convolutions):\n",
    "            for _ in range(n_residual_layers):\n",
    "                model += [\n",
    "                    SEANetResnetBlock2d(chnls, kernel_sizes=[1, 1],\n",
    "                                        dilations=[1, 1],\n",
    "                                        norm=block_norm, norm_params=norm_params,\n",
    "                                        activation=activation, activation_params=activation_params,\n",
    "                                        causal=causal, pad_mode=pad_mode, compress=compress, true_skip=true_skip)]\n",
    "\n",
    "            # Add downsampling layers\n",
    "            model += [\n",
    "                act(**activation_params),\n",
    "                StreamableConv2d(chnls, chnls // 2,\n",
    "                                 kernel_size=1, stride=1,\n",
    "                                 norm=block_norm, norm_kwargs=norm_params,\n",
    "                                 causal=causal, pad_mode=pad_mode),\n",
    "            ]\n",
    "            chnls //= 2\n",
    "\n",
    "        # flatten: results in output shape of (B, chnsl * freq_bins, T)\n",
    "        model += [nn.Flatten(1, 2)]\n",
    "        flattened_dim = chnls * freq_bins\n",
    "\n",
    "        # force desired latent size\n",
    "        model += [\n",
    "            act(**activation_params),\n",
    "            StreamableConv1d(flattened_dim, dimension, 1,\n",
    "                             norm='none' if self.disable_norm_outer_blocks == self.n_blocks else norm,\n",
    "                             norm_kwargs=norm_params, causal=causal, pad_mode=pad_mode)\n",
    "        ]\n",
    "\n",
    "        # take temporal strides\n",
    "        if len(self.temporal_ratios) > 0:\n",
    "            for i, ratio in enumerate(self.temporal_ratios):\n",
    "                block_norm = 'none' if self.disable_norm_outer_blocks >= i + 2 else norm\n",
    "                # Add residual layers\n",
    "                for j in range(n_residual_layers):\n",
    "                    model += [\n",
    "                        SEANetResnetBlock(dimension, kernel_sizes=[3, 1],\n",
    "                                        dilations=[1, 1],\n",
    "                                        norm=block_norm, norm_params=norm_params,\n",
    "                                        activation=activation, activation_params=activation_params,\n",
    "                                        causal=causal, pad_mode=pad_mode, compress=compress, true_skip=true_skip)]\n",
    "\n",
    "                # Add downsampling layers\n",
    "                model += [\n",
    "                    act(**activation_params),\n",
    "                    StreamableConv1d(dimension,dimension,\n",
    "                                    kernel_size=ratio * 2, stride=ratio,\n",
    "                                    norm=block_norm, norm_kwargs=norm_params,\n",
    "                                    causal=causal, pad_mode=pad_mode),\n",
    "                ]\n",
    "\n",
    "        if lstm:\n",
    "            model += [StreamableLSTM(dimension, num_layers=lstm)]\n",
    "\n",
    "        model += [\n",
    "            act(**activation_params),\n",
    "            StreamableConv1d(dimension, dimension, last_kernel_size,\n",
    "                             norm='none' if self.disable_norm_outer_blocks == self.n_blocks else norm,\n",
    "                             norm_kwargs=norm_params, causal=causal, pad_mode=pad_mode)\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class SEANetDecoder2d(nn.Module):\n",
    "    \"\"\"SEANet decoder.\n",
    "\n",
    "    Args:\n",
    "        channels (int): Audio channels.\n",
    "        dimension (int): Intermediate representation dimension.\n",
    "        n_filters (int): Base width for the model.\n",
    "        n_residual_layers (int): nb of residual layers.\n",
    "        ratios (Sequence[int]): kernel size and stride ratios.\n",
    "        activation (str): Activation function.\n",
    "        activation_params (dict): Parameters to provide to the activation function.\n",
    "        final_activation (str): Final activation function after all convolutions.\n",
    "        final_activation_params (dict): Parameters to provide to the activation function.\n",
    "        norm (str): Normalization method.\n",
    "        norm_params (dict): Parameters to provide to the underlying normalization used along with the convolution.\n",
    "        kernel_size (int): Kernel size for the initial convolution.\n",
    "        last_kernel_size (int): Kernel size for the initial convolution.\n",
    "        residual_kernel_size (int): Kernel size for the residual layers.\n",
    "        dilation_base (int): How much to increase the dilation with each layer.\n",
    "        causal (bool): Whether to use fully causal convolution.\n",
    "        pad_mode (str): Padding mode for the convolutions.\n",
    "        true_skip (bool): Whether to use true skip connection or a simple.\n",
    "            (streamable) convolution as the skip connection in the residual network blocks.\n",
    "        compress (int): Reduced dimensionality in residual branches (from Demucs v3).\n",
    "        lstm (int): Number of LSTM layers at the end of the encoder.\n",
    "        disable_norm_outer_blocks (int): Number of blocks for which we don't apply norm.\n",
    "            For the decoder, it corresponds to the N last blocks.\n",
    "        trim_right_ratio (float): Ratio for trimming at the right of the transposed convolution under the causal setup.\n",
    "            If equal to 1.0, it means that all the trimming is done at the right.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 3,\n",
    "                 ratios: tp.List[int] = [8, 5, 4, 2], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},\n",
    "                 final_activation: tp.Optional[str] = None, final_activation_params: tp.Optional[dict] = None,\n",
    "                 norm: str = 'none', norm_params: tp.Dict[str, tp.Any] = {}, kernel_size: int = 7,\n",
    "                 last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False,\n",
    "                 pad_mode: str = 'reflect', true_skip: bool = True, compress: int = 2, lstm: int = 0,\n",
    "                 disable_norm_outer_blocks: int = 0, trim_right_ratio: float = 1.0, frequency_bins: int = 512, \n",
    "                 num_1x1_convolutions: int = 2,\n",
    "                 temporal_ratios: tp.List[int] = [2]):\n",
    "        super().__init__()\n",
    "        self.num_1x1_convolutions = num_1x1_convolutions\n",
    "        self.temporal_ratios = temporal_ratios\n",
    "        self.dimension = dimension\n",
    "        self.channels = channels\n",
    "        self.n_filters = n_filters\n",
    "        self.frequency_bins = frequency_bins\n",
    "        assert self.frequency_bins == 512, \"model handles 512 frequency channels, change manually and in config file\" \n",
    "        self.ratios = ratios\n",
    "        del ratios\n",
    "        self.n_residual_layers = n_residual_layers\n",
    "        self.hop_length = np.prod(self.ratios)\n",
    "        self.n_blocks = len(self.ratios) + 2  # first and last conv + residual blocks\n",
    "        self.disable_norm_outer_blocks = disable_norm_outer_blocks\n",
    "        assert self.disable_norm_outer_blocks >= 0 and self.disable_norm_outer_blocks <= self.n_blocks, \\\n",
    "            \"Number of blocks for which to disable norm is invalid.\" \\\n",
    "            \"It should be lower or equal to the actual number of blocks in the network and greater or equal to 0.\"\n",
    "\n",
    "        act = getattr(nn, activation)\n",
    "        mult = int(2 ** len(self.ratios))\n",
    "        model: tp.List[nn.Module] = [\n",
    "            StreamableConv1d(dimension, dimension, kernel_size,\n",
    "                             norm='none' if self.disable_norm_outer_blocks == self.n_blocks else norm,\n",
    "                             norm_kwargs=norm_params, causal=causal, pad_mode=pad_mode)\n",
    "        ]\n",
    "                \n",
    "\n",
    "        if lstm:\n",
    "            model += [StreamableLSTM(dimension, num_layers=lstm)]\n",
    "\n",
    "        # reverse temporal strides \n",
    "        if len(temporal_ratios) > 0:\n",
    "            for i, ratio in enumerate(self.temporal_ratios):\n",
    "                block_norm = 'none' if self.disable_norm_outer_blocks >= self.n_blocks - (i + 1) else norm\n",
    "                # Add upsampling layers\n",
    "                model += [\n",
    "                    act(**activation_params),\n",
    "                    StreamableConvTranspose1d(dimension, dimension,\n",
    "                                            kernel_size=ratio * 2, stride=ratio,\n",
    "                                            norm=block_norm, norm_kwargs=norm_params,\n",
    "                                            causal=causal, trim_right_ratio=trim_right_ratio),\n",
    "                ]\n",
    "                # Add residual layers\n",
    "                for _ in range(n_residual_layers):\n",
    "                    model += [\n",
    "                        SEANetResnetBlock(dimension, kernel_sizes=[3, 1],\n",
    "                                        dilations=[1, 1],\n",
    "                                        activation=activation, activation_params=activation_params,\n",
    "                                        norm=block_norm, norm_params=norm_params, causal=causal,\n",
    "                                        pad_mode=pad_mode, compress=compress, true_skip=true_skip)]\n",
    "\n",
    "        chnls = n_filters * (2 ** (len(self.ratios) - self.num_1x1_convolutions))\n",
    "        freq_bins = frequency_bins // np.product(self.ratios)\n",
    "        \n",
    "\n",
    "        model += [\n",
    "            StreamableConv1d(dimension, chnls * freq_bins, kernel_size,\n",
    "                             norm='none' if self.disable_norm_outer_blocks == self.n_blocks else norm,\n",
    "                             norm_kwargs=norm_params, causal=causal, pad_mode=pad_mode)\n",
    "        ]\n",
    "        model += [nn.Unflatten(1, (int(chnls), int(freq_bins)))]\n",
    "\n",
    "        # scale channels using 1x1 convolutions\n",
    "        for _ in range(self.num_1x1_convolutions):\n",
    "            for _ in range(n_residual_layers):\n",
    "                model += [\n",
    "                    SEANetResnetBlock2d(chnls, kernel_sizes=[1, 1],\n",
    "                                        dilations=[1, 1],\n",
    "                                        norm=block_norm, norm_params=norm_params,\n",
    "                                        activation=activation, activation_params=activation_params,\n",
    "                                        causal=causal, pad_mode=pad_mode, compress=compress, true_skip=true_skip)]\n",
    "\n",
    "            # Add downsampling layers\n",
    "            model += [\n",
    "                act(**activation_params),\n",
    "                StreamableConv2d(chnls, chnls * 2,\n",
    "                                 kernel_size=1, stride=1,\n",
    "                                 norm=block_norm, norm_kwargs=norm_params,\n",
    "                                 causal=causal, pad_mode=pad_mode),\n",
    "            ]\n",
    "            chnls *= 2\n",
    "\n",
    "        # Upsample to raw audio scale\n",
    "        for i, ratio in enumerate(self.ratios):\n",
    "            block_norm = 'none' if self.disable_norm_outer_blocks >= self.n_blocks - (i + 1) else norm\n",
    "            # Add upsampling layers\n",
    "            model += [\n",
    "                act(**activation_params),\n",
    "                StreamableConvTranspose2d(chnls, chnls // 2,\n",
    "                                          kernel_size=ratio * 2, stride=ratio,\n",
    "                                          norm=block_norm, norm_kwargs=norm_params,\n",
    "                                          causal=causal, trim_right_ratio=trim_right_ratio),\n",
    "            ]\n",
    "            # Add residual layers\n",
    "            for j in range(n_residual_layers):\n",
    "                model += [\n",
    "                    SEANetResnetBlock2d(chnls // 2, kernel_sizes=[residual_kernel_size, 1],\n",
    "                                        dilations=[dilation_base ** j, 1],\n",
    "                                        activation=activation, activation_params=activation_params,\n",
    "                                        norm=block_norm, norm_params=norm_params, causal=causal,\n",
    "                                        pad_mode=pad_mode, compress=compress, true_skip=true_skip)]\n",
    "\n",
    "            chnls //= 2\n",
    "\n",
    "        # Add final layers\n",
    "        model += [\n",
    "            act(**activation_params),\n",
    "            StreamableConv2d(n_filters, 2, last_kernel_size,\n",
    "                             norm='none' if self.disable_norm_outer_blocks >= 1 else norm,\n",
    "                             norm_kwargs=norm_params, causal=causal, pad_mode=pad_mode)\n",
    "        ]\n",
    "        # Add optional final activation to decoder (eg. tanh)\n",
    "        if final_activation is not None:\n",
    "            final_act = getattr(nn, final_activation)\n",
    "            final_activation_params = final_activation_params or {}\n",
    "            model += [\n",
    "                final_act(**final_activation_params)\n",
    "            ]\n",
    "        # new_model = list()\n",
    "        # for m in model:\n",
    "        #     new_model.append(m)\n",
    "        #     new_model.append(Printer(type(m)))\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, z):\n",
    "        y = self.model(z)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:\n",
      "\n",
      "Latent:\n",
      "torch.Size([2, 128, 32])\n",
      "\n",
      "Decoder\n",
      "\n",
      "Output\n",
      "torch.Size([2, 2, 512, 64])\n",
      "Total trainable parameters: 52,095,026\n",
      "Total trainable parameters: 49,286,768\n",
      "tot: 101,381,794\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn((2, 2, 512, 64))\n",
    "print(\"Encoder:\")\n",
    "n_filt = 96\n",
    "n_res = 3\n",
    "encoder = SEANetEncoder2d(ratios=[2,2,2,2], n_filters=n_filt, lstm=2, n_residual_layers=n_res)\n",
    "z = encoder(x)\n",
    "print(\"\\nLatent:\")\n",
    "print(z.shape)\n",
    "print(\"\\nDecoder\")\n",
    "decoder = SEANetDecoder2d(ratios=[2,2,2,2], n_filters=n_filt, lstm=2, n_residual_layers=n_res)\n",
    "y = decoder(z)\n",
    "print(\"\\nOutput\")\n",
    "print(y.shape)\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_params:,}\")\n",
    "    return total_params\n",
    "\n",
    "dec_params = count_parameters(decoder)\n",
    "enc_params = count_parameters(encoder)\n",
    "print(f\"tot: {enc_params + dec_params:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encodec_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
